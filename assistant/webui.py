import gradio as gr
import os
os.environ["no_proxy"] = "localhost,127.0.0.1,::1"
from models import models
from vetorstore_search import vectorstores,vectorsearches
from langchain.agents import AgentType,initialize_agent
from langchain.chains import RetrievalQA
from langchain.chains.question_answering import load_qa_chain

tools = vectorsearches

def search_vector(*args,**kwargs):
    pass

def get_tools(selected_tools):
    tools = tools.keys()
    return tools

# ç›¸å…³æ“ä½œ
def fake_op(*args,**kwargs):
    return 'fake op'

def fast_searh(chatbot,query,select_vs):
    vectorstore = vectorstores[select_vs]
    retriever = vectorstore.as_retriever()
    docs = retriever.get_relevant_documents(query)
    ans = ''
    for i,doc in enumerate(docs):
        ans += f" {i} {doc.page_content}"
        ans += f""" This is detail: 
        ```
        {doc.metadata}
        ```
        \n\n
        """

    chatbot.append([query,ans])
    query = ''
    return chatbot, query
        
    

def answer_with_vectors(chatbot,query,temperature,top_p,max_length,select_vs,selected_llm):
    llm = models[selected_llm]
    vectorstore = vectorstores[select_vs]
    qa_chain = load_qa_chain(llm, chain_type="map_reduce")
    qa = RetrievalQA(combine_documents_chain=qa_chain, retriever=vectorstore.as_retriever())
    res = qa.run(query)
    chatbot.append([query,res])
    query = ''
    return chatbot, query

def answer_with_tools(chatbot, query,temperature,top_p,max_length,selected_tools,selected_llm):
    res = None
    llm = models[selected_llm]
    tools = get_tools(selected_tools)
    agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True,return_intermediate_steps=True)
    try:
        res = agent({"input":query})
        def format_interstep(res):
            ans = ''
            for i,(agent,thought) in enumerate(res["intermediate_steps"]):
                ans += f'step {i}\nnaction:{agent.log}\nthought {thought}\n'
            ans+=res["output"]
            return ans
    except Exception as e:
        res = f'Meet some problem, details: {e}'
    # è¿”å›ç»“æœ
    chatbot.append([query,res])
    query = ''
    return chatbot, query
    
def main():
    # web-ui éƒ¨åˆ†ä»£ç 
    block_css = """.importantButton {
        background: linear-gradient(45deg, #7e0570,#5d1c99, #6e00ff) !important;
        border: none !important;
    }
    .importantButton:hover {
        background: linear-gradient(45deg, #ff00e0,#8500ff, #6e00ff) !important;
        border: none !important;
    }"""
    webui_title = """
    # ğŸ‰Vibrant KnowlegeAssitantğŸ‰
    """
    default_theme_args = dict(
        font=["Source Sans Pro", 'ui-sans-serif', 'system-ui', 'sans-serif'],
        font_mono=['IBM Plex Mono', 'ui-monospace', 'Consolas', 'monospace'],
    )
    with gr.Blocks(css=block_css, theme=gr.themes.Default(**default_theme_args)) as demo:
    # with gr.Blocks() as demo:
        gr.Markdown(webui_title)
        with gr.Tab("AIå¼ºåŒ–æ£€ç´¢å¼•æ“"):
            init_message = f"""æ¬¢è¿ä½¿ç”¨AIå¼ºåŒ–æ£€ç´¢å¼•æ“ï¼Œè¯¥æ–¹æ¡ˆé‡‡ç”¨æœ€å…ˆè¿›çš„sentence_transformerså¯¹æ–‡æœ¬å†…å®¹è¿›è¡Œå‘é‡åŒ–è¡¨å¾ï¼Œèƒ½å¤Ÿæœ‰æ•ˆçš„æå‡æ£€ç´¢çš„ç›¸å…³æ€§."""
            with gr.Row():
                with gr.Column(scale=10):
                    chatbot = gr.Chatbot([[None, init_message]],
                                        elem_id="chat-box",
                                        show_label=False).style(height=750)
                    query = gr.Textbox(show_label=False,
                                    placeholder="è¯·è¾“å…¥æé—®å†…å®¹ï¼ŒæŒ‰å›è½¦è¿›è¡Œæäº¤").style(container=False)
                with gr.Column(scale=5):
                    select_vs = gr.Dropdown(list(vectorstores.keys()),
                                    label="è¯·é€‰æ‹©è¦åŠ è½½çš„çŸ¥è¯†åº“",
                                    interactive=True,
                                    value=list(vectorstores.keys())[0]
                                    )
                    top_p = gr.Slider(0, 1, value=0.95, label="top_p", info="Choose between 0 and 1.è®¾ç½®é˜ˆå€¼")
                    top_k = gr.Slider(0, 100, value=0.95, label="top_K", info="Choose between 0 and 1.è®¾ç½®é˜ˆå€¼")
                    
                query.submit(fast_searh,
                                    [chatbot,query,select_vs],
                                    [chatbot, query])
        with gr.Tab("AiçŸ¥è¯†åº“ï¼ˆQAï¼‰"):
            init_message = f"""æ¬¢è¿ä½¿ç”¨AiçŸ¥è¯†åº“ï¼Œè¯¥æ–¹æ¡ˆé‡‡ç”¨æœ€å…ˆè¿›çš„sentence_transformerså¯¹æ–‡æœ¬å†…å®¹è¿›è¡Œå‘é‡åŒ–è¡¨å¾ï¼Œèƒ½å¤Ÿæœ‰æ•ˆçš„æå‡æ£€ç´¢çš„ç›¸å…³æ€§ï¼Œä»è€Œå®ç°å¯¹å·²çŸ¥çŸ¥è¯†çš„å›ç­”
            """
            with gr.Row():
                with gr.Column(scale=10):
                    chatbot = gr.Chatbot([[None, init_message]],
                                        elem_id="chat-box",
                                        show_label=False).style(height=750)
                    query = gr.Textbox(show_label=False,
                                    placeholder="è¯·è¾“å…¥æé—®å†…å®¹ï¼ŒæŒ‰å›è½¦è¿›è¡Œæäº¤").style(container=False)
                with gr.Column(scale=5):
                    select_vs = gr.Dropdown(list(vectorstores.keys()),
                                    label="è¯·é€‰æ‹©è¦åŠ è½½çš„çŸ¥è¯†åº“",
                                    interactive=True,
                                    value=list(vectorstores.keys())[0]
                                    )
                    temperature = gr.Slider(0, 1, value=0.7, label="temperature", info="Choose between 0 and 1")
                    top_p = gr.Slider(0, 1, value=0.95, label="top_p", info="Choose between 0 and 1")
                    max_length  = gr.Slider(0, 2048, value=512, label="max_length", info="max length of generate words.")
                    selected_llm = gr.Dropdown(list(models.keys()),label='Model',info='available LLM Model',value=list(models.keys())[0])
                    query.submit(answer_with_vectors,
                                    [chatbot,query,temperature,top_p,max_length,select_vs,selected_llm],
                                    [chatbot, query])
        
        with gr.Tab("Aiæ¨ç†é—®ç­”"):
            init_message = f"""æ¬¢è¿ä½¿ç”¨Aiæ¨ç†é—®ç­”ã€‚
            Aiæ™ºèƒ½é—®ç­”æ˜¯åŸºäºLLMæ¨¡å‹å’ŒRe-Actæ¨ç†æ¡†æ¶çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚å®ƒèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å·²æœ‰çš„æ£€ç´¢å·¥å…·ï¼Œå…¶æ¨¡å¼ä¸ºï¼šact then rethinkã€‚
            å…·ä½“æ¥è¯´ï¼šæé—®ï¼šé£Ÿç®¡ç™Œæœ‰å“ªäº›æ²»ç–—æ–¹æ¡ˆã€‚
            åœ¨ç¬¬ä¸€æ­¥ï¼š ä»–ä¼šå…ˆå­¦ä¹ ï¼š ä»€ä¹ˆæ˜¯é£Ÿç®¡ç™Œï¼Ÿé€šè¿‡æ£€ç´¢è·å–çŸ¥è¯†ã€‚
            ç¬¬äºŒæ­¥ï¼š å†æ¬¡å‘é—®ï¼Œé£Ÿç®¡ç™Œæœ‰å“ªäº›æ²»ç–—æ–¹æ¡ˆï¼Ÿé€šè¿‡æ£€ç´¢è·å–çŸ¥è¯†ã€‚
            å¦‚æ­¤å¾ªç¯ä¸‹å»ï¼Œç›´åˆ°èƒ½å¤Ÿå›ç­”é—®é¢˜ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆçš„å‡å°‘å¹»æƒ³ï¼Œå‡å°‘äº‹å®æ€§é”™è¯¯ã€‚
            ä½†æ˜¯æ¨ç†è¿‡ç¨‹æ—¶ä¼šå—åˆ°ç”Ÿæˆå‚æ•°çš„å½±å“ï¼Œå¦‚æœé‡åˆ°äº†å¥‡æ€ªçš„æ¨ç†ï¼Œå¯ä»¥é€šè¿‡ä¼˜åŒ–è‡ªå·±çš„é—®é¢˜å’Œæ¨¡å‹çš„æ¨ç†å‚æ•°ä»¥è¾¾åˆ°æƒ³è¦çš„ç›®çš„ã€‚
            """
            with gr.Row():
                with gr.Column(scale=10):
                    chatbot = gr.Chatbot([[None, init_message],],
                                        elem_id="chat-box",
                                        show_label=False).style(height=750)
                    query = gr.Textbox(show_label=False,
                                    placeholder="è¯·è¾“å…¥æé—®å†…å®¹ï¼ŒæŒ‰å›è½¦è¿›è¡Œæäº¤").style(container=False)
                    
                with gr.Column(scale=5):
                    temperature = gr.Slider(0, 1, value=0.7, label="temperature", info="Choose between 0 and 1")
                    top_p = gr.Slider(0, 1, value=0.95, label="top_p", info="Choose between 0 and 1")
                    max_length  = gr.Slider(0, 2048, value=0.95, label="top_p", info="Choose between 0 and 1")
                    selected_tools = gr.CheckboxGroup(['pubmed','arvix','google_search'],label='Tools',info='available tools for ai')
                    selected_llm = gr.Dropdown(list(models.keys()),label='Model',info='available LLM Model',value=list(models.keys())[0])
                query.submit(answer_with_tools,
                            [chatbot, query,temperature,top_p,max_length,selected_tools,selected_llm],
                            [chatbot, query]
                            )
    demo.queue(concurrency_count=3).launch(
            server_name='0.0.0.0',
            server_port=6006,
            show_api=False,
            share=False,
            inbrowser=False)

if __name__ == '__main__':
    main()